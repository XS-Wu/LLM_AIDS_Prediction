{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61965306",
   "metadata": {},
   "source": [
    "import os, math, json, random, warnings, gc, time, re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from contextlib import contextmanager\n",
    "from typing import Dict, Any\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, confusion_matrix,\n",
    "                             f1_score, precision_score, recall_score, roc_auc_score,\n",
    "                             classification_report)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, get_linear_schedule_with_warmup\n",
    "from peft import LoraConfig, get_peft_model, get_peft_model_state_dict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"PYTHONUNBUFFERED\"] = \"1\"\n",
    "\n",
    "MODEL_PATH = r\"G:/models/Qwen3-0.6B\"\n",
    "DATA_PATH = \"data.csv\"\n",
    "SYSTEM_PROMPT = \"你是一个传染病预测专家。请判断下一个月相对于本月的艾滋病发病数是否出现“上涨≥10%”。\"\n",
    "RES_DIR = Path(\"results\"); RES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TH_UP_PCT = 0.10\n",
    "CLS2TEXT = {0:\"未上涨10%\", 1:\"上涨≥10%\"}\n",
    "NUM_CLASSES = 2\n",
    "ENSEMBLE_SEEDS = [3407]\n",
    "FAST_DEBUG = False\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "MAX_LENGTH = 64\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "DROPOUT = 0.30\n",
    "EPOCHS_TRAIN = 20\n",
    "GRAD_CLIP = 1.0\n",
    "WARMUP_RATIO = 0.10\n",
    "EARLY_PATIENCE = 9     \n",
    "\n",
    "FIXED_THR_PROB = 0.40 \n",
    "\n",
    "USE_SAMPLE_WEIGHTS = False\n",
    "POS_WEIGHT_MODE = \"sqrt\"\n",
    "FOCAL_ALPHA_POS = 0.5\n",
    "FOCAL_ALPHA_NEG = 0.5\n",
    "FOCAL_GAMMA = 0.0            \n",
    "\n",
    "CALIBRATOR = \"platt\"  \n",
    "\n",
    "SPLIT_RATIOS = (0.7, 0.2, 0.1)\n",
    "\n",
    "# 设备\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "USE_AMP = torch.cuda.is_available()\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "ATTN_IMPL = \"sdpa\"\n",
    "\n",
    "def set_seed(s=3407):\n",
    "    random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(s)\n",
    "\n",
    "def eval_metrics_binary(y_true, prob_pos, y_pred, *, lite=False):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_pred = np.asarray(y_pred).astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1m = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    f1w = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    f1b = f1_score(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    bal = balanced_accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    tn, fp, fn, tp = (cm.ravel() if cm.size==4 else (0,0,0,0))\n",
    "    spec = tn/(tn+fp) if (tn+fp)>0 else 0.0\n",
    "    out = {\n",
    "        \"accuracy\":float(acc),\n",
    "        \"macro_f1\":float(f1m),\n",
    "        \"weighted_f1\":float(f1w),\n",
    "        \"binary_f1\":float(f1b),\n",
    "        \"balanced_accuracy\":float(bal),\n",
    "        \"precision\":float(prec),\n",
    "        \"recall\":float(rec),\n",
    "        \"specificity\":float(spec),\n",
    "        \"confusion_matrix\": cm.tolist()\n",
    "    }\n",
    "    if not lite:\n",
    "        try:\n",
    "            out[\"roc_auc\"] = float(roc_auc_score(y_true, prob_pos))\n",
    "        except Exception:\n",
    "            out[\"roc_auc\"] = None\n",
    "        out[\"report\"] = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "    return out\n",
    "\n",
    "def metrics_at_thr(y_true, prob_pos, thr, *, lite=True):\n",
    "    y_pred = (np.asarray(prob_pos) >= float(thr)).astype(int)\n",
    "    return eval_metrics_binary(y_true, prob_pos, y_pred, lite=lite), y_pred\n",
    "\n",
    "def build_calibrator_from_state(state: Dict[str, Any]):\n",
    "    kind = state.get(\"kind\", \"temperature\")\n",
    "    if kind == \"temperature\":\n",
    "        T = float(state[\"T\"])\n",
    "        def calibrate(s):\n",
    "            s = np.asarray(s).reshape(-1)\n",
    "            z = s/float(T)\n",
    "            p = 1/(1+np.exp(-z))\n",
    "            return np.clip(p, 1e-6, 1-1e-6)\n",
    "        return calibrate\n",
    "    elif kind == \"platt\":\n",
    "        a = float(state[\"a\"]); b = float(state[\"b\"])\n",
    "        def calibrate(s):\n",
    "            s = np.asarray(s).reshape(-1)\n",
    "            z = a*s + b\n",
    "            p = 1/(1+np.exp(-z))\n",
    "            return np.clip(p, 1e-6, 1-1e-6)\n",
    "        return calibrate\n",
    "    else: \n",
    "        xs = np.asarray(state[\"xs\"], dtype=float)\n",
    "        ys = np.asarray(state[\"ys\"], dtype=float)\n",
    "        def calibrate(s):\n",
    "            s = np.asarray(s).reshape(-1)\n",
    "            p = np.interp(s, xs, ys, left=ys[0], right=ys[-1])\n",
    "            return np.clip(p, 1e-6, 1-1e-6)\n",
    "        return calibrate\n",
    "\n",
    "def calibrator_fit(scores, y_true, kind=\"temperature\"):\n",
    "    scores = np.asarray(scores).reshape(-1)\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    if kind == \"isotonic\":\n",
    "        from sklearn.isotonic import IsotonicRegression\n",
    "        iso = IsotonicRegression(out_of_bounds=\"clip\").fit(scores, y_true)\n",
    "        xs = np.unique(np.sort(scores))\n",
    "        ys = iso.predict(xs)\n",
    "        state = {\"kind\":\"isotonic\",\"xs\": xs.tolist(), \"ys\": ys.tolist()}\n",
    "        return build_calibrator_from_state(state), state\n",
    "    elif kind == \"platt\":\n",
    "        lr_cls = LogisticRegression(C=1.0, solver=\"lbfgs\").fit(scores.reshape(-1,1), y_true)\n",
    "        a = float(lr_cls.coef_[0,0]); b = float(lr_cls.intercept_[0])\n",
    "        state = {\"kind\":\"platt\",\"a\": a, \"b\": b}\n",
    "        return build_calibrator_from_state(state), state\n",
    "    else: \n",
    "        Ts = np.linspace(0.5, 3.0, 51)\n",
    "        def nll(T):\n",
    "            z = scores/float(T)\n",
    "            p = 1/(1+np.exp(-z)); eps=1e-7\n",
    "            return -np.mean(y_true*np.log(p+eps)+(1-y_true)*np.log(1-p+eps))\n",
    "        Tbest = float(Ts[np.argmin([nll(T) for T in Ts])])\n",
    "        state = {\"kind\":\"temperature\",\"T\": Tbest}\n",
    "        return build_calibrator_from_state(state), state\n",
    "\n",
    "def prob_thr_to_score_thr_prob_cal(state: Dict[str, Any], prob_thr: float) -> float:\n",
    "    kind = state.get(\"kind\", \"temperature\")\n",
    "    if kind == \"temperature\":\n",
    "        T = float(state[\"T\"])\n",
    "        logit = math.log(prob_thr/(1-prob_thr))\n",
    "        return float(T*logit)\n",
    "    elif kind == \"platt\":\n",
    "        a = float(state[\"a\"]); b = float(state[\"b\"])\n",
    "        logit = math.log(prob_thr/(1-prob_thr))\n",
    "        return float((logit - b)/a)\n",
    "    else:\n",
    "        return float(\"nan\")\n",
    "\n",
    "def prob_thr_to_score_thr(scores, calibrate_func, prob_thr: float, state: Dict[str, Any]) -> float:\n",
    "    s = prob_thr_to_score_thr_prob_cal(state, prob_thr)\n",
    "    if not (np.isfinite(s)):\n",
    "        s_sorted = np.sort(np.unique(np.asarray(scores).reshape(-1)))\n",
    "        p_sorted = calibrate_func(s_sorted)\n",
    "        idx = np.searchsorted(p_sorted, prob_thr, side=\"left\")\n",
    "        if idx <= 0:  return float(s_sorted[0]) - 1e-6\n",
    "        if idx >= len(s_sorted): return float(s_sorted[-1]) + 1e-6\n",
    "        s0, s1 = s_sorted[idx-1], s_sorted[idx]\n",
    "        p0, p1 = p_sorted[idx-1], p_sorted[idx] if idx < len(p_sorted) else p_sorted[-1]\n",
    "        if p1 == p0: return float(s1)\n",
    "        t = (prob_thr - p0) / (p1 - p0)\n",
    "        s = float(s0 + t*(s1 - s0))\n",
    "    return s\n",
    "\n",
    "def read_csv_any(path: str) -> pd.DataFrame:\n",
    "    for enc in [\"utf-8-sig\",\"gb18030\",\"utf-8\"]:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except:\n",
    "            pass\n",
    "    raise RuntimeError(f\"无法读取 {path}\")\n",
    "\n",
    "def ym_cn(p):\n",
    "    y,m = str(p).split(\"-\"); return f\"{int(y)}年{int(m)}月\"\n",
    "\n",
    "def logratio(a, b):\n",
    "    return math.log1p(a) - math.log1p(b)\n",
    "\n",
    "raw = read_csv_any(DATA_PATH)\n",
    "\n",
    "cols = {c: re.sub(r\"\\s+\", \"\", str(c)) for c in raw.columns}\n",
    "rev = {v:k for k,v in cols.items()}\n",
    "month_col = rev.get(\"月份\", \"月份\")\n",
    "region_col = rev.get(\"地区\", \"地区\")\n",
    "cases_col = rev.get(\"发病数\", \"发病数\")\n",
    "index_col = rev.get(\"全部\", \"全部\")\n",
    "\n",
    "raw = raw.rename(columns={month_col:\"月份\", region_col:\"地区\", cases_col:\"发病数\", index_col:\"全部\"})\n",
    "\n",
    "raw[\"月份\"] = raw[\"月份\"].astype(str).str.slice(0,7)\n",
    "raw[\"period\"] = pd.PeriodIndex(raw[\"月份\"], freq=\"M\")\n",
    "raw[\"地区_std\"] = raw[\"地区\"].astype(str).str.strip()\n",
    "raw[\"发病数\"] = pd.to_numeric(raw[\"发病数\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "raw[\"全部\"] = pd.to_numeric(raw[\"全部\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "raw = raw[(raw[\"period\"]>=pd.Period(\"2011-01\",\"M\")) & (raw[\"period\"]<=pd.Period(\"2020-12\",\"M\"))].copy()\n",
    "raw = raw.sort_values([\"地区_std\",\"period\"]).reset_index(drop=True)\n",
    "\n",
    "raw[\"CASE_lag12\"] = raw.groupby(\"地区_std\")[\"发病数\"].shift(12)\n",
    "raw[\"IDX_lag12\"] = raw.groupby(\"地区_std\")[\"全部\"].shift(12)\n",
    "\n",
    "reg_months = raw.groupby(\"地区_std\")[\"period\"].nunique().sort_values(ascending=False)\n",
    "REGIONS = reg_months.index.tolist()\n",
    "REG2ID = {rg:i for i,rg in enumerate(REGIONS)}\n",
    "\n",
    "rows=[]\n",
    "for rg, g in raw.groupby(\"地区_std\", sort=False):\n",
    "    g = g.sort_values(\"period\").reset_index(drop=True)\n",
    "    for i in range(len(g)-3):\n",
    "        t2,t1,t0,tP1 = g.iloc[i], g.iloc[i+1], g.iloc[i+2], g.iloc[i+3]\n",
    "        if (pd.Period(t2[\"period\"],\"M\")+1) != pd.Period(t1[\"period\"],\"M\"): continue\n",
    "        if (pd.Period(t1[\"period\"],\"M\")+1) != pd.Period(t0[\"period\"],\"M\"): continue\n",
    "        if (pd.Period(t0[\"period\"],\"M\")+1) != pd.Period(tP1[\"period\"],\"M\"): continue\n",
    "\n",
    "        y_t, y_tp1 = int(t0[\"发病数\"]), int(tP1[\"发病数\"])\n",
    "        rel = (y_tp1 - y_t) / max(y_t, 1e-6)\n",
    "        y = 1 if rel >= TH_UP_PCT else 0\n",
    "\n",
    "        month = int(str(t0[\"period\"])[-2:])\n",
    "        sinm, cosm = np.sin(2*np.pi*month/12.0), np.cos(2*np.pi*month/12.0)\n",
    "\n",
    "        lag12_y = float(t0[\"CASE_lag12\"]) if pd.notna(t0[\"CASE_lag12\"]) else 0.0\n",
    "        lag12_idx = float(t0[\"IDX_lag12\"]) if pd.notna(t0[\"IDX_lag12\"]) else 0.0\n",
    "\n",
    "        yoy_case = (math.log1p(float(t0[\"发病数\"])) - math.log1p(lag12_y)) if lag12_y>0 else 0.0\n",
    "        yoy_idx = (math.log1p(float(t0[\"全部\"])) - math.log1p(lag12_idx)) if lag12_idx>0 else 0.0\n",
    "\n",
    "        row = {\n",
    "            \"地区_std\":rg,\"date_feature\":str(t0[\"period\"]),\"date_target\":str(tP1[\"period\"]),\n",
    "            \"t_month\":month,\"y_cls\":y,\n",
    "            \"x_case_t\":float(t0[\"发病数\"]),\n",
    "            \"x_case_t_prev\":float(t1[\"发病数\"]),\n",
    "            \"x_case_t_prev2\":float(t2[\"发病数\"]),\n",
    "            \"x_idx_t\":float(t0[\"全部\"]),\n",
    "            \"x_idx_t_prev\":float(t1[\"全部\"]),\n",
    "            \"x_idx_t_prev2\":float(t2[\"全部\"]),\n",
    "            \"logratio_case_10\":logratio(float(t0[\"发病数\"]),float(t1[\"发病数\"])),\n",
    "            \"logratio_case_21\":logratio(float(t1[\"发病数\"]),float(t2[\"发病数\"])),\n",
    "            \"logratio_idx_10\":logratio(float(t0[\"全部\"]),float(t1[\"全部\"])),\n",
    "            \"logratio_idx_21\":logratio(float(t1[\"全部\"]),float(t2[\"全部\"])),\n",
    "            \"y_t_lag12\":float(lag12_y),\n",
    "            \"x_idx_t_lag12\":float(lag12_idx),\n",
    "            \"yoy_logratio_case\":float(yoy_case),\n",
    "            \"yoy_logratio_idx\":float(yoy_idx),\n",
    "            \"month_sin\":float(sinm),\n",
    "            \"month_cos\":float(cosm),\n",
    "        }\n",
    "        row[\"input_text\"] = (\n",
    "            f\"System: {SYSTEM_PROMPT}\\n\"\n",
    "            f\"User: Question: （地区={rg}）已知{ym_cn(t2['period'])}的艾滋病发病数为{int(t2['发病数'])}、搜索指数为{int(round(t2['全部']))}；\"\n",
    "            f\"{ym_cn(t1['period'])}的艾滋病发病数为{int(t1['发病数'])}、搜索指数为{int(round(t1['全部']))}；\"\n",
    "            f\"{ym_cn(t0['period'])}的艾滋病发病数为{int(t0['发病数'])}、搜索指数为{int(round(t0['全部']))}。\"\n",
    "            f\"请判断下一月相对于{ym_cn(t0['period'])}是否出现“上涨≥10%”。Answer:\"\n",
    "        )\n",
    "        rows.append(row)\n",
    "\n",
    "data = pd.DataFrame(rows).sort_values([\"date_feature\",\"地区_std\"]).reset_index(drop=True)\n",
    "assert not data.empty, \"没有可用样本\"\n",
    "\n",
    "FEAT_COLS = [\n",
    " \"x_case_t\",\"x_case_t_prev\",\"x_case_t_prev2\",\n",
    " \"x_idx_t\",\"x_idx_t_prev\",\"x_idx_t_prev2\",\n",
    " \"logratio_case_10\",\"logratio_case_21\",\n",
    " \"logratio_idx_10\",\"logratio_idx_21\",\n",
    " \"y_t_lag12\",\"x_idx_t_lag12\",\n",
    " \"yoy_logratio_case\",\"yoy_logratio_idx\",\n",
    " \"month_sin\",\"month_cos\"\n",
    "]\n",
    "\n",
    "months = sorted(data[\"date_target\"].unique().tolist())\n",
    "M = len(months); assert M>=3\n",
    "m_train = int(math.floor(M*SPLIT_RATIOS[0]))\n",
    "m_val = int(math.floor(M*SPLIT_RATIOS[1]))\n",
    "if m_train+m_val>=M:\n",
    "    m_val = max(1, M-m_train-1)\n",
    "\n",
    "train_months = months[:m_train]\n",
    "val_months   = months[m_train:m_train+m_val]\n",
    "test_months  = months[m_train+m_val:]\n",
    "\n",
    "df_tr = data[data[\"date_target\"].isin(train_months)].reset_index(drop=True)\n",
    "df_vl = data[data[\"date_target\"].isin(val_months )].reset_index(drop=True)\n",
    "df_te = data[data[\"date_target\"].isin(test_months )].reset_index(drop=True)\n",
    "\n",
    "def fit_regionwise_stats(train_df: pd.DataFrame, cols, group_col=\"地区_std\"):\n",
    "    stats = {}\n",
    "    for rg, g in train_df.groupby(group_col):\n",
    "        stats[rg] = {}\n",
    "        for c in cols:\n",
    "            mu = float(g[c].mean()); sd = float(g[c].std()) or 1.0\n",
    "            stats[rg][c] = (mu, sd)\n",
    "    gmu = {c: float(train_df[c].mean()) for c in cols}\n",
    "    gsd = {c: float(train_df[c].std()) or 1.0 for c in cols}\n",
    "    return stats, gmu, gsd\n",
    "\n",
    "def apply_regionwise_standardize(df: pd.DataFrame, cols, stats, gmu, gsd, group_col=\"地区_std\"):\n",
    "    g = df.copy()\n",
    "    rgs = g[group_col].astype(str).values\n",
    "    for c in cols:\n",
    "        vals = g[c].astype(float).values\n",
    "        out = np.empty_like(vals, dtype=float)\n",
    "        for i, rg in enumerate(rgs):\n",
    "            mu, sd = stats.get(rg, {}).get(c, (gmu[c], gsd[c]))\n",
    "            out[i] = (vals[i] - mu) / (sd if sd!=0 else 1.0)\n",
    "        g[c] = out\n",
    "    return g\n",
    "\n",
    "stats_rg, gmu, gsd = fit_regionwise_stats(df_tr, FEAT_COLS)\n",
    "trN = apply_regionwise_standardize(df_tr, FEAT_COLS, stats_rg, gmu, gsd)\n",
    "vlN = apply_regionwise_standardize(df_vl, FEAT_COLS, stats_rg, gmu, gsd)\n",
    "teN = apply_regionwise_standardize(df_te, FEAT_COLS, stats_rg, gmu, gsd)\n",
    "\n",
    "x_scaler = StandardScaler().fit(trN[FEAT_COLS].astype(\"float32\").values)\n",
    "def scale_X(df):\n",
    "    return x_scaler.transform(df[FEAT_COLS].astype(\"float32\").values).astype(\"float32\")\n",
    "\n",
    "# =============== Dataset/Loader ===============\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, local_files_only=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "class TxtBinDataset(Dataset):\n",
    "    def __init__(self, df, feats_scaled, tokenizer):\n",
    "        self.texts = df[\"input_text\"].tolist()\n",
    "        self.feats = feats_scaled.astype(\"float32\")\n",
    "        self.y = df[\"y_cls\"].astype(\"int64\").values\n",
    "        self.region_id = np.array([REG2ID[s] for s in df[\"地区_std\"].astype(str).tolist()], dtype=np.int64)\n",
    "        self.meta = df.reset_index(drop=True)\n",
    "        enc = tokenizer(self.texts, truncation=True, padding=True, max_length=MAX_LENGTH, pad_to_multiple_of=8)\n",
    "        self.enc = {k: torch.tensor(v, dtype=torch.long) for k,v in enc.items()}\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i):\n",
    "        it = {k:v[i] for k,v in self.enc.items()}\n",
    "        it[\"feats\"] = torch.tensor(self.feats[i], dtype=torch.float32)\n",
    "        it[\"y\"] = torch.tensor(self.y[i], dtype=torch.long)\n",
    "        it[\"region_id\"] = torch.tensor(self.region_id[i], dtype=torch.long)\n",
    "        return it\n",
    "\n",
    "def collate_fn(b):\n",
    "    return {\n",
    "        \"input_ids\": torch.stack([x[\"input_ids\"] for x in b]),\n",
    "        \"attention_mask\": torch.stack([x[\"attention_mask\"]for x in b]),\n",
    "        \"feats\": torch.stack([x[\"feats\"] for x in b]),\n",
    "        \"y\": torch.stack([x[\"y\"] for x in b]),\n",
    "        \"region_id\": torch.stack([x[\"region_id\"] for x in b]),\n",
    "    }\n",
    "\n",
    "def make_loader(df, feats_scaled, tokenizer, batch_size, shuffle):\n",
    "    ds = TxtBinDataset(df, feats_scaled, tokenizer)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn,\n",
    "                    num_workers=0, pin_memory=False)\n",
    "    return ds, dl\n",
    "\n",
    "# =============== LoRA ===============\n",
    "def get_lora_base(train_lora=True):\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH, trust_remote_code=True,\n",
    "        dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        low_cpu_mem_usage=True, local_files_only=True,\n",
    "        device_map={\"\":0} if torch.cuda.is_available() else None,\n",
    "        attn_implementation=ATTN_IMPL\n",
    "    )\n",
    "    base.config.use_cache = False\n",
    "    base.config.output_hidden_states = True\n",
    "    peft_cfg = LoraConfig(r=4, lora_alpha=16, lora_dropout=0.05, target_modules=[\"q_proj\",\"v_proj\",\"o_proj\"],\n",
    "                          bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "    lora = get_peft_model(base, peft_cfg)\n",
    "    for n,p in lora.named_parameters():\n",
    "        p.requires_grad = (\"lora_\" in n) and bool(train_lora)\n",
    "    return lora\n",
    "\n",
    "class QwenLoRABinary(nn.Module):\n",
    "    def __init__(self, feat_dim: int, dropout=0.3, train_lora=True, train_head=True, num_regions=None, reg_emb_dim=16):\n",
    "        super().__init__()\n",
    "        self.backbone = get_lora_base(train_lora=train_lora)\n",
    "        hid = self.backbone.config.hidden_size\n",
    "        self.reg_emb = nn.Embedding(num_regions if num_regions else len(REGIONS), reg_emb_dim)\n",
    "        self.feat_mlp = nn.Sequential(\n",
    "            nn.Linear(feat_dim+reg_emb_dim,128), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(128,128), nn.ReLU()\n",
    "        )\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fuse = nn.Linear(hid+128, hid)\n",
    "        self.act = nn.ReLU()\n",
    "        self.out = nn.Linear(hid,1)\n",
    "        for p in self.feat_mlp.parameters(): p.requires_grad = bool(train_head)\n",
    "        for p in self.fuse.parameters(): p.requires_grad = bool(train_head)\n",
    "        for p in self.out.parameters(): p.requires_grad = bool(train_head)\n",
    "        nn.init.normal_(self.out.weight, std=1e-3); nn.init.zeros_(self.out.bias)\n",
    "        self.register_buffer(\"pos_w\", torch.ones(()))\n",
    "    def forward(self, input_ids, attention_mask, feats, region_id):\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                            output_hidden_states=True, use_cache=False, return_dict=True)\n",
    "        last = out.hidden_states[-1]\n",
    "        mask = attention_mask.unsqueeze(-1).float()\n",
    "        pooled = (last*mask).sum(1) / mask.sum(1).clamp(min=1e-6)\n",
    "        pooled = self.drop(pooled)\n",
    "        reg = self.reg_emb(region_id)\n",
    "        featv = self.feat_mlp(torch.cat([feats, reg], -1))\n",
    "        h = self.act(self.fuse(torch.cat([pooled, featv], -1)))\n",
    "        logit = self.out(self.drop(h)).squeeze(-1)\n",
    "        return {\"logit\": logit}\n",
    "\n",
    "def get_trainable_state_dict(m: nn.Module):\n",
    "    sd={}\n",
    "    try:\n",
    "        lora_sd = get_peft_model_state_dict(m.backbone)\n",
    "        for k,v in lora_sd.items():\n",
    "            sd[f\"backbone.{k}\"]=v.detach().cpu()\n",
    "    except Exception:\n",
    "        pass\n",
    "    for name,mod in [(\"reg_emb\",m.reg_emb),(\"feat_mlp\",m.feat_mlp),(\"fuse\",m.fuse),(\"out\",m.out)]:\n",
    "        for k,v in mod.state_dict().items():\n",
    "            sd[f\"{name}.{k}\"]=v.detach().cpu()\n",
    "    if hasattr(m,\"pos_w\"):\n",
    "        sd[\"pos_w\"]=m.pos_w.detach().cpu()\n",
    "    return sd\n",
    "\n",
    "def load_trainable_state_dict(m: nn.Module, sd: dict):\n",
    "    lora_sd = {k.replace(\"backbone.\",\"\"):v for k,v in sd.items() if k.startswith(\"backbone.\")}\n",
    "    if len(lora_sd):\n",
    "        m.backbone.load_state_dict(lora_sd, strict=False)\n",
    "    for name,module in [(\"reg_emb\",m.reg_emb),(\"feat_mlp\",m.feat_mlp),(\"fuse\",m.fuse),(\"out\",m.out)]:\n",
    "        sub = {k.replace(f\"{name}.\",\"\"):v for k,v in sd.items() if k.startswith(f\"{name}.\")}\n",
    "        if len(sub): module.load_state_dict(sub, strict=False)\n",
    "    if \"pos_w\" in sd and hasattr(m,\"pos_w\"):\n",
    "        m.pos_w.copy_(sd[\"pos_w\"].to(m.pos_w.dtype))\n",
    "\n",
    "def binary_focal_bce_with_logits(logits,y,pos_weight=None,gamma=FOCAL_GAMMA,\n",
    "                                 alpha_pos=FOCAL_ALPHA_POS,alpha_neg=FOCAL_ALPHA_NEG,\n",
    "                                 sample_weight=None):\n",
    "    bce = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "        logits,y.float(),pos_weight=pos_weight,reduction=\"none\")\n",
    "    p = torch.sigmoid(logits); pt = y*p + (1-y)*(1-p)\n",
    "    alpha_t = alpha_pos*y + alpha_neg*(1-y)\n",
    "    if gamma > 0:\n",
    "        focal = alpha_t * ((1-pt).clamp(min=1e-6)**gamma) * bce\n",
    "    else:\n",
    "        focal = alpha_t * bce\n",
    "    if sample_weight is not None:\n",
    "        focal = focal * sample_weight\n",
    "    return focal.mean()\n",
    "\n",
    "@contextmanager\n",
    "def model_in_eval(m: nn.Module):\n",
    "    was_training = m.training\n",
    "    m.eval()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        if was_training:\n",
    "            m.train()\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_validation(model, loader, desc=\"eval\"):\n",
    "    losses=[]; probs=[]; scores=[]; ys=[]\n",
    "    with model_in_eval(model):\n",
    "        for b in tqdm(loader, desc=desc, leave=False):\n",
    "            ids=b[\"input_ids\"].to(device); mask=b[\"attention_mask\"].to(device)\n",
    "            feats=b[\"feats\"].to(device); y=b[\"y\"].to(device); rid=b[\"region_id\"].to(device)\n",
    "            with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
    "                logit = model(ids, mask, feats=feats, region_id=rid)[\"logit\"]\n",
    "                loss = binary_focal_bce_with_logits(logit,y,pos_weight=model.pos_w)\n",
    "            losses.append(float(loss.item()))\n",
    "            probs.append(torch.sigmoid(logit).detach().cpu().numpy())\n",
    "            scores.append(logit.detach().cpu().numpy())\n",
    "            ys.append(y.detach().cpu().numpy())\n",
    "    return (np.mean(losses) if len(losses) else 0.0,\n",
    "            np.concatenate(probs,0), np.concatenate(scores,0), np.concatenate(ys,0))\n",
    "\n",
    "def compute_pos_weight(cnt, mode=POS_WEIGHT_MODE):\n",
    "    ratio = float(cnt[0] / max(cnt[1],1.0))\n",
    "    if mode == \"sqrt\": return math.sqrt(ratio)\n",
    "    if mode == \"log\":  return math.log1p(ratio)\n",
    "    return ratio\n",
    "\n",
    "def train_one_seed(seed, trN, vlN, teN):\n",
    "    set_seed(seed)\n",
    "    model = QwenLoRABinary(len(FEAT_COLS), dropout=DROPOUT,\n",
    "                           train_lora=True, train_head=True,\n",
    "                           num_regions=len(REGIONS), reg_emb_dim=16).to(device)\n",
    "\n",
    "    if FAST_DEBUG:\n",
    "        trN = trN.iloc[: max(256, min(2048, len(trN)))].reset_index(drop=True)\n",
    "        vlN = vlN.iloc[: max(128, min(1024, len(vlN)))].reset_index(drop=True)\n",
    "        teN = teN.iloc[: max(128, min(1024, len(teN)))].reset_index(drop=True)\n",
    "\n",
    "    ds_tr, dl_tr = make_loader(trN, scale_X(trN), tokenizer, BATCH_SIZE, shuffle=True)\n",
    "    ds_vl, dl_vl = make_loader(vlN, scale_X(vlN), tokenizer, BATCH_SIZE, shuffle=False)\n",
    "    ds_te, dl_te = make_loader(teN, scale_X(teN), tokenizer, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    ytr = trN[\"y_cls\"].values.astype(int)\n",
    "    cnt = np.bincount(ytr, minlength=2).astype(float)\n",
    "    pos_weight = compute_pos_weight(cnt, POS_WEIGHT_MODE)\n",
    "    model.pos_w = torch.tensor(float(pos_weight), dtype=torch.float32, device=device)\n",
    "\n",
    "    print(f\"pos_weight(+1)={float(pos_weight):.4f} | class_counts={cnt.tolist()} | USE_SAMPLE_WEIGHTS={USE_SAMPLE_WEIGHTS}\", flush=True)\n",
    "    print(f\"Loader sizes | train={len(dl_tr)} val={len(dl_vl)} test={len(dl_te)} | epochs={EPOCHS_TRAIN if not FAST_DEBUG else 1}\", flush=True)\n",
    "\n",
    "    if USE_SAMPLE_WEIGHTS:\n",
    "        inv = cnt.sum()/np.clip(cnt,1.0,None); inv = inv/inv.mean()\n",
    "        cls_weight_vec = torch.tensor(inv, dtype=torch.float32, device=device)\n",
    "    else:\n",
    "        cls_weight_vec = None\n",
    "\n",
    "    opt = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad],\n",
    "                            lr=LR, weight_decay=WEIGHT_DECAY, fused=False)\n",
    "    steps_total = max(1, (len(dl_tr) if len(dl_tr)>0 else 1) * (1 if FAST_DEBUG else EPOCHS_TRAIN))\n",
    "    sch = get_linear_schedule_with_warmup(opt, int(steps_total*WARMUP_RATIO), steps_total)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "\n",
    "    best_payload = None\n",
    "    best_val_score = -1.0  \n",
    "    pat = 0\n",
    "    per_epoch = []\n",
    "    epochs_run = 1 if FAST_DEBUG else EPOCHS_TRAIN\n",
    "\n",
    "    for ep in range(1, epochs_run+1):\n",
    "        model.train()\n",
    "        total=0.0; step=0\n",
    "        pbar = tqdm(dl_tr, desc=f\"Epoch {ep:02d} [train]\", leave=False)\n",
    "        for b in pbar:\n",
    "            ids=b[\"input_ids\"].to(device); mask=b[\"attention_mask\"].to(device)\n",
    "            feats=b[\"feats\"].to(device); y=b[\"y\"].to(device); rid=b[\"region_id\"].to(device)\n",
    "            sw = cls_weight_vec[y] if (cls_weight_vec is not None and USE_SAMPLE_WEIGHTS) else None\n",
    "            with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
    "                logit = model(ids, mask, feats=feats, region_id=rid)[\"logit\"]\n",
    "                loss = binary_focal_bce_with_logits(\n",
    "                    logit, y, pos_weight=model.pos_w,\n",
    "                    gamma=FOCAL_GAMMA,\n",
    "                    alpha_pos=FOCAL_ALPHA_POS, alpha_neg=FOCAL_ALPHA_NEG,\n",
    "                    sample_weight=sw\n",
    "                ) / GRAD_ACCUM_STEPS\n",
    "            scaler.scale(loss).backward()\n",
    "            step += 1\n",
    "            if step % GRAD_ACCUM_STEPS == 0:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_([p for p in model.parameters() if p.requires_grad], GRAD_CLIP)\n",
    "                scaler.step(opt); scaler.update(); opt.zero_grad(set_to_none=True)\n",
    "                if sch: sch.step()\n",
    "            total += float(loss.item())*GRAD_ACCUM_STEPS\n",
    "            if step % 10 == 0:\n",
    "                pbar.set_postfix(loss=f\"{(total/max(1,step)):.4f}\")\n",
    "\n",
    "        tr_loss = total/max(1,len(dl_tr))\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        vl_loss, pr_vl_raw, sc_vl, y_vl = run_validation(model, dl_vl, desc=f\"Epoch {ep:02d} [val]\")\n",
    "        calibrate, cal_state = calibrator_fit(sc_vl, y_vl, kind=CALIBRATOR)\n",
    "        pr_vl = calibrate(sc_vl)\n",
    "\n",
    "        thr_prob = FIXED_THR_PROB\n",
    "        thr_score = prob_thr_to_score_thr(sc_vl, calibrate, thr_prob, cal_state)\n",
    "\n",
    "        tr_loss_eval, pr_tr_raw, sc_tr, y_tr = run_validation(model, dl_tr, desc=f\"Epoch {ep:02d} [train-eval]\")\n",
    "        te_loss_eval, pr_te_raw, sc_te, y_te = run_validation(model, dl_te, desc=f\"Epoch {ep:02d} [test]\")\n",
    "        pr_tr = calibrate(sc_tr); pr_te = calibrate(sc_te)\n",
    "\n",
    "        m_tr,_ = metrics_at_thr(y_tr, pr_tr, thr_prob, lite=False)\n",
    "        m_vl,_ = metrics_at_thr(y_vl, pr_vl, thr_prob, lite=False)\n",
    "        m_te,_ = metrics_at_thr(y_te, pr_te, thr_prob, lite=False)\n",
    "\n",
    "        val_auc = m_vl.get(\"roc_auc\", -1.0) or -1.0\n",
    "        val_score = np.mean([\n",
    "            m_vl[\"accuracy\"],\n",
    "            m_vl[\"binary_f1\"],\n",
    "            m_vl[\"macro_f1\"],\n",
    "            m_vl[\"precision\"],\n",
    "            m_vl[\"recall\"],\n",
    "            val_auc\n",
    "        ])\n",
    "\n",
    "        print(\n",
    "            f\"\\nEpoch {ep:02d} | tr_loss={tr_loss:.4f} | val_loss={vl_loss:.4f} | \"\n",
    "            f\"AUC_val={val_auc:.3f} | THR(fixed)={thr_prob:.3f} | VAL_score_no_spec={val_score:.3f}\",\n",
    "            flush=True\n",
    "        )\n",
    "        print(\n",
    "            f\" TRAIN Acc={m_tr['accuracy']:.3f} F1={m_tr['binary_f1']:.3f} \"\n",
    "            f\"P={m_tr['precision']:.3f} R={m_tr['recall']:.3f} \"\n",
    "            f\"Spec={m_tr['specificity']:.3f} BalAcc={m_tr['balanced_accuracy']:.3f}\", flush=True\n",
    "        )\n",
    "        print(\n",
    "            f\" VAL   Acc={m_vl['accuracy']:.3f} F1={m_vl['binary_f1']:.3f} \"\n",
    "            f\"P={m_vl['precision']:.3f} R={m_vl['recall']:.3f} \"\n",
    "            f\"Spec={m_vl['specificity']:.3f} BalAcc={m_vl['balanced_accuracy']:.3f}\", flush=True\n",
    "        )\n",
    "        print(\n",
    "            f\" TEST  Acc={m_te['accuracy']:.3f} F1={m_te['binary_f1']:.3f} \"\n",
    "            f\"P={m_te['precision']:.3f} R={m_te['recall']:.3f} \"\n",
    "            f\"Spec={m_te['specificity']:.3f} BalAcc={m_te['balanced_accuracy']:.3f}\", flush=True\n",
    "        )\n",
    "\n",
    "        per_epoch.append({\n",
    "            \"epoch\": ep,\n",
    "            \"thr_prob\": float(thr_prob),\n",
    "            \"thr_score\": float(thr_score),\n",
    "            \"calibrator\": CALIBRATOR,\n",
    "            \"val_score_no_spec\": float(val_score),\n",
    "            \"train\": m_tr,\n",
    "            \"val\": m_vl,\n",
    "            \"test\": m_te\n",
    "        })\n",
    "\n",
    "        if val_score > best_val_score + 1e-4:\n",
    "            best_val_score = val_score\n",
    "\n",
    "            snap_tr = {\n",
    "                \"scores\": sc_tr.astype(np.float32),\n",
    "                \"labels\": y_tr.astype(np.int64),\n",
    "                \"meta\": ds_tr.meta.to_dict(orient=\"list\")\n",
    "            }\n",
    "            snap_vl = {\n",
    "                \"scores\": sc_vl.astype(np.float32),\n",
    "                \"labels\": y_vl.astype(np.int64),\n",
    "                \"meta\": ds_vl.meta.to_dict(orient=\"list\")\n",
    "            }\n",
    "            snap_te = {\n",
    "                \"scores\": sc_te.astype(np.float32),\n",
    "                \"labels\": y_te.astype(np.int64),\n",
    "                \"meta\": ds_te.meta.to_dict(orient=\"list\")\n",
    "            }\n",
    "\n",
    "            payload = {\n",
    "                \"trainable\": get_trainable_state_dict(model),\n",
    "                \"thr_prob\": float(thr_prob),\n",
    "                \"thr_score\": float(thr_score),\n",
    "                \"calibrator_state\": cal_state,\n",
    "                \"epoch\": ep,\n",
    "                \"snapshots\": {\"train\": snap_tr, \"val\": snap_vl, \"test\": snap_te},\n",
    "                \"note\": \"BEST_BY_VAL_MULTI_NO_SPEC_FIXED_THR\"\n",
    "            }\n",
    "            best_payload = payload\n",
    "            torch.save(payload, RES_DIR/\"best_by_val_score_no_spec.pt\")\n",
    "            pat = 0\n",
    "        else:\n",
    "            pat += 1\n",
    "            if pat >= EARLY_PATIENCE and not FAST_DEBUG:\n",
    "                print(f\"早停：patience={EARLY_PATIENCE}\", flush=True)\n",
    "                break\n",
    "\n",
    "        torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "    if best_payload is None:\n",
    "        payload = {\n",
    "            \"trainable\": get_trainable_state_dict(model),\n",
    "            \"thr_prob\": float(thr_prob),\n",
    "            \"thr_score\": float(thr_score),\n",
    "            \"calibrator_state\": cal_state,\n",
    "            \"epoch\": ep,\n",
    "            \"snapshots\": {\"train\": snap_tr, \"val\": snap_vl, \"test\": snap_te},\n",
    "            \"note\": \"FALLBACK_LAST_EPOCH\"\n",
    "        }\n",
    "        torch.save(payload, RES_DIR/\"best_by_val_score_no_spec.pt\")\n",
    "        best_payload = payload\n",
    "\n",
    "    print(f\"[SAVE] 使用 best_by_val_score_no_spec @ epoch {best_payload['epoch']} VAL_score_no_spec={best_val_score:.4f}\", flush=True)\n",
    "    return RES_DIR/\"best_by_val_score_no_spec.pt\", per_epoch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise SystemExit(\"未检测到 GPU 环境。\")\n",
    "torch.cuda.set_device(0)\n",
    "print(\"===== 开始训练=====\", flush=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, local_files_only=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "ckpt_path, per_epoch_log = train_one_seed(ENSEMBLE_SEEDS[0], trN, vlN, teN)\n",
    "\n",
    "def load_payload(ckpt_path: Path):\n",
    "    payload = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    cal_state = payload.get(\"calibrator_state\", {\"kind\": \"temperature\", \"T\": 1.0})\n",
    "    thr_prob = float(payload.get(\"thr_prob\", FIXED_THR_PROB))\n",
    "    thr_score = float(payload.get(\"thr_score\", 0.0))\n",
    "    epoch = int(payload.get(\"epoch\", -1))\n",
    "    snaps = payload.get(\"snapshots\", {})\n",
    "    return cal_state, thr_prob, thr_score, epoch, snaps\n",
    "\n",
    "CAL_STATE, DECISION_THR_PROB, DECISION_THR_SCORE, SELECTED_EPOCH, SNAPSHOTS = load_payload(ckpt_path)\n",
    "calibrate = build_calibrator_from_state(CAL_STATE)\n",
    "\n",
    "def finalize_from_snapshot(snap: Dict[str, Any], thr_prob: float, thr_score: float):\n",
    "    meta = pd.DataFrame(snap[\"meta\"])\n",
    "    scores = np.asarray(snap[\"scores\"]).reshape(-1)\n",
    "    labels = np.asarray(snap[\"labels\"]).astype(int).reshape(-1)\n",
    "    prob = calibrate(scores)\n",
    "    yhat = (scores >= float(thr_score)).astype(int)\n",
    "    m_full = eval_metrics_binary(labels, prob, yhat, lite=False)\n",
    "    out_meta = meta.copy()\n",
    "    out_meta[\"prob_pos\"] = prob\n",
    "    out_meta[\"score_logit\"] = scores\n",
    "    out_meta[\"pred_cls\"] = yhat\n",
    "    out_meta[\"pred_text\"] = out_meta[\"pred_cls\"].map(CLS2TEXT)\n",
    "    out_meta[\"true_text\"] = out_meta[\"y_cls\"].map(CLS2TEXT)\n",
    "    out_meta[\"thr_used_prob\"] = float(thr_prob)\n",
    "    out_meta[\"thr_used_score\"] = float(thr_score)\n",
    "    return m_full, out_meta\n",
    "\n",
    "m_tr,  meta_tr  = finalize_from_snapshot(SNAPSHOTS[\"train\"], DECISION_THR_PROB, DECISION_THR_SCORE)\n",
    "m_vl,  meta_vl  = finalize_from_snapshot(SNAPSHOTS[\"val\"]  , DECISION_THR_PROB, DECISION_THR_SCORE)\n",
    "m_te,  meta_te  = finalize_from_snapshot(SNAPSHOTS[\"test\"] , DECISION_THR_PROB, DECISION_THR_SCORE)\n",
    "\n",
    "with open(RES_DIR/\"per_epoch_log.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(per_epoch_log, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(RES_DIR/\"metrics.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"train\": m_tr, \"val\": m_vl, \"test\": m_te,\n",
    "        \"selected_regions\": REGIONS,\n",
    "        \"feature_cols\": FEAT_COLS,\n",
    "        \"class_mapping\": CLS2TEXT,\n",
    "        \"threshold_pct_for_positive\": TH_UP_PCT,\n",
    "        \"decision_threshold_on_val_prob\": float(DECISION_THR_PROB),\n",
    "        \"decision_threshold_on_val_score\": float(DECISION_THR_SCORE),\n",
    "        \"calibrator_state\": CAL_STATE,\n",
    "        \"use_sample_weights\": USE_SAMPLE_WEIGHTS,\n",
    "        \"pos_weight_mode\": POS_WEIGHT_MODE,\n",
    "        \"seeds\": ENSEMBLE_SEEDS,\n",
    "        \"attn_impl\": ATTN_IMPL,\n",
    "        \"calibrator\": CALIBRATOR,\n",
    "        \"dataloader\": {\"num_workers\": 0, \"pin_memory\": False},\n",
    "        \"split_by\": \"date_target_month\",\n",
    "        \"split_months\": {\"train\": train_months, \"val\": val_months, \"test\": test_months},\n",
    "        \"selected_epoch\": SELECTED_EPOCH,\n",
    "        \"select_strategy\": \"best_val_multi_no_spec_fixed_thr\",\n",
    "        \"fixed_thr_prob\": FIXED_THR_PROB,\n",
    "        \"data_path\": DATA_PATH,\n",
    "        \"data_columns_mapping\": {\"月份\":\"period\",\"地区\":\"地区_std\",\"发病数\":\"cases\",\"全部\":\"index_all\"}\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with pd.ExcelWriter(RES_DIR/\"cls_predictions.xlsx\", engine=\"xlsxwriter\") as writer:\n",
    "    meta_tr.to_excel(writer, sheet_name=\"train\", index=False)\n",
    "    meta_vl.to_excel(writer, sheet_name=\"val\"  , index=False)\n",
    "    meta_te.to_excel(writer, sheet_name=\"test\" , index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
